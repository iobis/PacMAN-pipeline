---
title: "Report of the PacMAN pipeline"
author: "OBIS secretariat"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    number_sections: TRUE
    toc: true
params:
  config: ""
---

# PacMAN pipeline report

Congratulations! You have finished running the PacMAN pipeline. In this report you will find the information of all the main steps run with your sequences, and the statistics related to how it went.

```{r, include=FALSE}
library(yaml)
library(knitr)
library(phyloseq)
library(ggplot2)
library(dplyr)
library(stringr)

config <- read_yaml(params$config)
```

```{r, echo=FALSE, results='asis'}
cat("Specifically, this is the report for the project: `", config$PROJECT, "`    And the pipeline run: `", config$RUN, "`.    ", sep = "")

if (R.utils::isAbsolutePath(config$SAMPLE_SET)) {
  sample_set_file <- config$SAMPLE_SET
} else {
  sample_set_file <- file.path("../..", config$SAMPLE_SET)
}
sample_set <- read.csv(sample_set_file)
sample_ids <- unique(sample_set$sample.id)

cat("And the samples that were run in this analysis are:\n\n")
for (id in sample_ids) {
  cat("- ", id, "\n")
}
```

## Quality control

Quality control of the raw samples was done so that any issues in the final analysis can possibly be traced back to issues in the original data.

```{r, echo=F, results='asis'}
cat("Visual representation of the results of the QC on the raw data can be found in an html report in:  \n results/", config$PROJECT, "/samples/multiqc_", config$RUN, ".html  \n", sep = "")

cat("Generally, the sequences in the samples had the following composition:  \n  \n")

raw_stats_path <- paste0("../../results/", config$PROJECT, "/samples/multiqc_", config$RUN, "_data/multiqc_general_stats.txt")

raw_stats <- read.table(raw_stats_path, header = TRUE, sep = "\t")
colnames(raw_stats) <- gsub("FastQC_mqc.generalstats.fastqc.", "", colnames(raw_stats))
colnames(raw_stats) <- gsub("_", " ", colnames(raw_stats))
raw_stats[,c(2,5)] <- round(raw_stats[,c(2,5)], digits = 1)

kable(raw_stats)
```

## Sequence trimming

As you can most likely see, there are several issues with the raw sequences. Therefore, the sequences were trimmed and cleaned-up prior to the analysis of sequence variants and the taxonomic composition of the samples.

The first step of the process was to trim and quality control the samples using [fastp](https://github.com/OpenGene/fastp) ([Shifu Chen 2023](https://doi.org/10.1002/imt2.107)).

And the results for each sample in reads, and in percentages of original reads:

```{r, echo=F, warning=F}
# Loop across all sample_ids and collect results to table
trimResults <- data.frame(Read1BeforeFiltering = NA, Read2BeforeFiltering = NA, Read1AfterFiltering = NA, Read2AfterFiltering = NA)

for (i in 1:length(sample_ids)) {
  trim_report <- paste0("../../results/", config$PROJECT, "/samples/", sample_ids[i], "/fastp/", sample_ids[i], "_log.txt")

  file <- file(trim_report, "r")
  line <- readLines(file)

  total_reads <- line[grep(pattern = "total reads:", line)]
  numbers <- gsub("[^0-9]+", "", total_reads)
  as.numeric(numbers)

  trimResults[i,] <- numbers
  rownames(trimResults)[i] <- sample_ids[i]
}

kable(trimResults)
```

## Cutadapt

The next step in the process ran cutadapt on all samples. All analyses were done in single-mode, forward and reverse reads were analysed separately.
This is done so that reads that do not have a pair are not discarded at this stage. 

The command(s) that was used was:

```{r, echo=F}
#cut_report <- capture.output(cat("../PacMAN-pipeline-backup/results/", config$PROJECT, "/runs/", config$RUN, "/06-report/cutadapt/", sample_ids[1], "_log.txt", sep = ""))
cut_report_1P <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/cutadapt/", sample_ids[1], "_1P_log.txt")
readLines(cut_report_1P, n = 2)
cut_report_2P <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/cutadapt/", sample_ids[1], "_2P_log.txt")
readLines(cut_report_2P, n = 2)
```


### Forward reads

```{r, echo=F, warning=F}
cutResults <- data.frame(TotalReadsProcessed = NA, ReadwithPrimer = NA, ReverseComplemented=NA, ReadsTooShort = NA, ReadsWritten = NA, BasePairsProcessed = NA, BasePairsWritten = NA)

for (i in 1:length(sample_ids)) {
  cut_report <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/cutadapt/", sample_ids[i], "_1P_log.txt")
  text <- readLines(cut_report)

  if (any(grepl("^===", text))) {
    rows <- grep("^===", text)
    report <- text[c((rows[1]+2):(rows[2]-2))]
    sample <- report %>% str_extract_all("\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b(?![^(]*\\))") %>% unlist %>% {gsub(',','',.)} %>% as.numeric

    #Not all of the reports list ReadsTooShort, so add for those that don't a 0
    if (!grepl("short", report[4])) {
      sample <- append(sample, 0, after = 2)
    }
    cutResults[i,] <- sample[!is.na(sample)]
    rownames(cutResults)[i] <- paste(sample_ids[i],"1P")
  } else {
    cutResults[i,] <- rep(0,6)
    rownames(cutResults)[i] <- paste(sample_ids[i],"1P")
  }
}

kable(cutResults)
cutResults_Percentage <- round(cutResults[,c("ReadwithPrimer", "ReadsWritten")] / cutResults$TotalReadsProcessed * 100, digits = 2)
cutResults_Percentage$BasePairsWritten <- round(cutResults$BasePairsWritten / cutResults$BasePairsProcessed * 100, digits = 2)
kable(cutResults_Percentage)
```

### Reverse reads

```{r, echo=F, warning=F}
cutResults <- data.frame(TotalReadsProcessed = NA, ReadwithPrimer = NA, ReverseComplemented=NA, ReadsTooShort = NA, ReadsWritten = NA, BasePairsProcessed = NA, BasePairsWritten = NA)

for (i in 1:length(sample_ids)) {
  cut_report <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/cutadapt/", sample_ids[i], "_2P_log.txt")
  text <- readLines(cut_report)

  if (any(grepl("^===", text))) {
    rows <- grep("^===", text)
    report <- text[c((rows[1]+2):(rows[2]-2))]
    sample <- report %>% str_extract_all("\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b(?![^(]*\\))") %>% unlist %>% {gsub(',','',.)} %>% as.numeric

    #Not all of the reports list ReadsTooShort, so add for those that don't a 0
    if (!grepl("short", report[4])) {
      sample <- append(sample, 0, after = 2)
    }
    cutResults[i,] <- sample[!is.na(sample)]
    rownames(cutResults)[i] <- paste(sample_ids[i],"2P")
  } else {
    cutResults[i,] <- rep(0,6)
    rownames(cutResults)[i] <- paste(sample_ids[i],"2P")
  }
}

kable(cutResults)
cutResults_Percentage <- round(cutResults[,c("ReadwithPrimer", "ReadsWritten")] / cutResults$TotalReadsProcessed * 100, digits = 2)
cutResults_Percentage$BasePairsWritten <- round(cutResults$BasePairsWritten / cutResults$BasePairsProcessed * 100, digits = 2)
kable(cutResults_Percentage)
```

### Unpaired forward reads

For forward unpaired reads they were:

```{r, echo=F, warning=F}
cutResults <- data.frame(TotalReadsProcessed = NA, ReadwithPrimer = NA, ReverseComplemented=NA, ReadsTooShort = NA, ReadsWritten = NA, BasePairsProcessed = NA, BasePairsWritten = NA)

for (i in 1:length(sample_ids)) {
  cut_report <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/cutadapt/", sample_ids[i], "_1U_log.txt")
  text <- readLines(cut_report)

  if (any(grepl("^===", text))) {
    rows <- grep("^===", text)
    report <- text[c((rows[1]+2):(rows[2]-2))]
    sample <- report %>% str_extract_all("\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b(?![^(]*\\))") %>% unlist %>% {gsub(',','',.)} %>% as.numeric

    #Not all of the reports list ReadsTooShort, so add for those that don't a 0
    if (!grepl("short", report[4])) {
      sample <- append(sample, 0, after = 2)
    }
    cutResults[i,] <- sample[!is.na(sample)]
    rownames(cutResults)[i] <- paste(sample_ids[i],"1U")
  } else {
    cutResults[i,] <- rep(0,6)
    rownames(cutResults)[i] <- paste(sample_ids[i],"1U")
  }
}

kable(cutResults)
cutResults_Percentage <- round(cutResults[,c("ReadwithPrimer", "ReadsWritten")] / cutResults$TotalReadsProcessed * 100, digits = 2)
cutResults_Percentage$BasePairsWritten <- round(cutResults$BasePairsWritten / cutResults$BasePairsProcessed * 100, digits = 2)
kable(cutResults_Percentage)
```

### Unpaired reverse

For reverse unpaired reads they were:

```{r, echo=F, warning=F}
cutResults <- data.frame(TotalReadsProcessed = NA, ReadwithPrimer = NA, ReverseComplemented=NA, ReadsTooShort = NA, ReadsWritten = NA, BasePairsProcessed = NA, BasePairsWritten = NA)

for (i in 1:length(sample_ids)) {
  cut_report <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/cutadapt/", sample_ids[i], "_2U_log.txt")
  text <- readLines(cut_report)

  if (any(grepl("^===", text))) {
    rows <- grep("^===", text)
    report <- text[c((rows[1]+2):(rows[2]-2))]
    sample <- report %>% str_extract_all("\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b(?![^(]*\\))") %>% unlist %>% {gsub(',','',.)} %>% as.numeric

    #Not all of the reports list ReadsTooShort, so add for those that don't a 0
    if (!grepl("short", report[4])) {
      sample <- append(sample, 0, after = 2)
    }
    cutResults[i,] <- sample[!is.na(sample)]
    rownames(cutResults)[i] <- paste(sample_ids[i],"2U")
  } else {
    cutResults[i,] <- rep(0,6)
    rownames(cutResults)[i] <- paste(sample_ids[i],"2U")
  }
}

kable(cutResults)
cutResults_Percentage <- round(cutResults[,c("ReadwithPrimer", "ReadsWritten")] / cutResults$TotalReadsProcessed * 100, digits = 2)
cutResults_Percentage$BasePairsWritten <- round(cutResults$BasePairsWritten / cutResults$BasePairsProcessed * 100, digits = 2)
kable(cutResults_Percentage)
```

Currently the unpaired reads that are kept at this step are not analysed further. Keep this in mind if there is a large amount of unpaired forward and reverse reads.

## ASV inference with dada2

Next the reads without adapters and primers, were analysed using the dada2 pipeline. This pipeline evaluates the error rates in reads, and based on the results defines amplicon sequence variants (ASVs).

**Note!** At this moment in this step we are only analysing the reads that are still paired after sequence trimming. We plan to include the analysis of unpaired reads also in the future.

### Filter and trim

The first step of the dada2 pipeline is to perform more filtering and trimming based on sequence quality.

The parameters used in this run were:

```{r, echo=F}
cat("filterAndTrim(filesForw, filtFs, filesRev, filtRs,
  truncLen = c(" ,config$DADA2$filterAndTrim$Trunc_len_f,", " ,config$DADA2$filterAndTrim$Trunc_len_r,"),truncQ=" ,config$DADA2$filterAndTrim$TruncQ,",
  trimRight = ",config$DADA2$filterAndTrim$Trim_right,", trimLeft = ",config$DADA2$filterAndTrim$Trim_left,",
  maxLen = ",config$DADA2$filterAndTrim$maxLen,", minLen = ",config$DADA2$filterAndTrim$minLen,",
  maxN = ",config$DADA2$filterAndTrim$maxN,", minQ = ",config$DADA2$filterAndTrim$minQ,", maxEE = ",config$DADA2$filterAndTrim$MaxEE,",
  rm.phix = ",config$DADA2$filterAndTrim$Rm.phix,", orient.fwd = ",config$DADA2$filterAndTrim$orient.fwd,",
  matchIDs = ",config$DADA2$filterAndTrim$matchIDs,", id.sep = ",config$DADA2$filterAndTrim$id.sep,", id.field = ",config$DADA2$filterAndTrim$id.field,",
  compress = ",config$DADA2$filterAndTrim$compress,", multithread = ",config$DADA2$filterAndTrim$multithread,",
  n = ",config$DADA2$filterAndTrim$n,", OMP = ",config$DADA2$filterAndTrim$OMP,",
  verbose = ",config$DADA2$filterAndTrim$verbose,"
)", sep="")
```

The quality profile of all analysed paired reads before trimming (Forward reads in left panel, while reverse reads are in right panel):

```{r, echo=FALSE, out.width="49%", out.height="20%", fig.cap="caption", fig.show='hold', fig.align='center'}
knitr::include_graphics(c(paste0("../../results/", config$PROJECT, "/runs/", config$RUN,"/06-report/dada2/aggregate_quality_profiles_filesForw.png"), paste0("../../results/", config$PROJECT, "/runs/", config$RUN,"/06-report/dada2/aggregate_quality_profiles_filesRev.png")))
```

And after trimming it was:

```{r, echo=FALSE, out.width="49%", out.height="20%", fig.cap="caption", fig.show='hold', fig.align='center'}
if (config$meta$sequencing$lib_layout=="Paired") {
  knitr::include_graphics(c(paste0("../../results/", config$PROJECT, "/runs/", config$RUN,"/06-report/dada2/aggregate_quality_profiles_paired_filtered_forward.png"), paste0("../../results/", config$PROJECT, "/runs/", config$RUN,"/06-report/dada2/aggregate_quality_profiles_paired_filtered_reverse.png")))
}
```

In case unpaired reads were kept after trimming and primer removal, quality profiles for unpaired reads can also be found in:

```{r, echo=FALSE, out.width="49%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
message(paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/dada2/aggregate_quality_profiles_filesForw_single.png"))
message(paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/dada2/aggregate_quality_profiles_filesRev_single.png"))
message(paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/dada2/aggregate_quality_profiles_filtered_unpaired_forward.png"))
message(paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/dada2/aggregate_quality_profiles_fitlered_unpaired_reverse.png"))
```

### Asv inference

The following step of the dada2 pipeline was define the ASVs from these trimmed reads.

First error rates were learned for both forward and reverse paired reads separately as well as any unpaired remaining reads which survived filtering, with the following command:

```{r, echo=F}
cat("learnErrors(filtFs, multithread = ", config$DADA2$learnERRORS$multithread, ", nbases = ", config$DADA2$learnERRORS$nbases, ", randomize = ", config$DADA2$learnERRORS$randomize, ",MAX_CONSIST = ", config$DADA2$learnERRORS$MAX_CONSIST,", OMEGA_C = ", config$DADA2$learnERRORS$OMEGA_C, ", verbose = ", config$DADA2$learnERRORS$verbose, ")", sep = "")
```

The forward and reverse error profiles were the following (in absolute numbers and in percentage of analysed reads):

```{r, echo=FALSE, out.width="49%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(paste0("../../results/", config$PROJECT, "/runs/", config$RUN,"/06-report/dada2/error_profile_filtFs.png"), paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/dada2/error_profile_filtRs.png")))
```

Next the reads were dereplicated with the following command:

```{r, echo=F}
cat("derepFastq(filtFs, n = ", config$DADA2$derepFastq$n, ")", sep = "")
```

The error rates were then used to infer ASVs from the dereplicated sequences using the following command for both forward and reverse reads separately:

```{r, echo=F}
cat("dada(derepFs, err = errF, selfConsist = ", config$DADA2$dada$selfConsist,", pool = ", config$DADA2$dada$pool, ", priors = ", config$DADA2$dada$priors, ",
  multithread = ", config$DADA2$learnERRORS$multithread,")", sep = "")
```

In case reads were overlapping (include merge in the config file), sequence pairs were combined using:

```{r, echo=F}
cat("mergePairs(dadaFs, derepFs, dadaRs, derepRs,
  minOverlap = ", config$DADA2$mergePairs$minOverlap,", maxMismatch = ", config$DADA2$mergePairs$maxMismatch, ", returnRejects = ", config$DADA2$mergePairs$returnRejects, ",
  propagateCol = ", config$DADA2$mergePairs$propagateCol,", justConcatenate = ", config$DADA2$mergePairs$justConcatenate, ", trimOverhang = ", config$DADA2$mergePairs$trimOverhang, ")", sep = "")
```

If returnRejects was selected, also the returned unpaired sequences were kept in the final tables.

Otherwise, the dereplicated sequences were kept without merging.

Also any sequences unpaired forward or reverse sequences were added to the sequence tables at this stage.

All single reverse sequences (unpaired either in trimmomatic or here, or if pairs were not merged), were reverse complemented for the bowtie2 algorithm.

And finally, chimeras were removed with:

```{r, echo=F}
cat("removeBimeraDenovo(seqtab, method = ", config$DADA2$removeBimeraDenovo$method, ", multithread = ", config$DADA2$learnERRORS$multithread, ")", sep="")
```

The amount of reads analysed in each step and remaining at the end of the pipeline were:

```{r, echo=F}
dada2_report <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/dada2/dada2_stats.txt")

dada2_stats <- read.table(dada2_report, header = TRUE)
kable(dada2_stats)
```

## Assigning taxonomy

### RDP

Next, taxonomy was assigned to the inferred reads using rdp. It's important to note that 

The command used for the rdp search was the following:

```{r, echo=F}
cat("rdp_classifier -Xmx16g classify -t", config$Rdp$location_rdp_ref, " -o  results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/rdp/" , config$Rdp$rdp_ref_name, "_rdp.output",
    " -q results/", config$PROJECT, "/runs/", config$RUN, "/03-dada2/rep-seqs.fna", sep = "")
```

Additionally a vsearch local alignment was made to a reference database to find matches and their ids.

```{r, echo=F}
cat("vsearch --usearch_global results/", config$PROJECT, "/runs/", config$RUN, "/03-dada2/rep-seqs.fna  --db", config$Vsearch$location_vsearch_db, " --id ", config$Vsearch$pident,
        " --blast6out results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/vsearch/", config$Vsearch$vsearch_db_name, "_vsearch_blast.b6 --threads 4  --output_no_hits --query_cov ", config$Vsearch$query_cov,
        " --maxaccepts 100 --maxrejects 100 --maxhits 10 ",
        " --lcaout results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/vsearch/", config$Vsearch$vsearch_db_name, "_vsearch_lcaout.tab  --lca_cutoff 0.6 --notrunclabels ",
        "2> --lcaout results/", config$PROJECT, "/runs/", config$RUN, "/06-report/vsearch/vsearch_", config$Vsearch$vsearch_db_name, ".log ", sep = "")
```

This command indicates that from a maximum of 10 hits that all meet the criteria (identity and query coverage), the final result is a consensus taxonomy that over 60% of these hits agree on. 
This lca workflow is meant to filter out hits, were there are multiple conflicting high-identity hits.

The vsearch log indicates how many matches were made with this search:

```{r, echo=F}
vsearch_log <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/06-report/vsearch/vsearch_", config$Vsearch$vsearch_db_name, ".log" )

file <- file(vsearch_log, "r")
line <- readLines(file)

total_matches<- line[grep(pattern="Matching unique query sequences:", line)]

cat(total_matches, "with an identity cutoff of", config$Vsearch$pident, "a query coverage of", config$Vsearch$query_cov, "and an lca cutoff of 0.6")
```

Finally all results are stored in the output files after a filtering step.

```{r, echo=F}
cat("The rdp results were then filtered based off of the user defined probability cutoff:", config$Rdp$cutoff)
```

The rdp results after filtering are stored as the taxonomic lineage, while the full results with probability, and the vsearch hits are stored in the field "identificationRemarks", and are included in the final Darwin Core files.

### Blast against the full ncbi-nt

If chosen by the user (Blast: include: TRUE in the config file) the unknown sequences that did not have a match (results either "", or only "Eukaryota"), were further analysed with blastn.

```{R, echo=F}
cat("The unknown sequences defined in this way can be found in: /results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/unknown_asvs.txt", sep="")

unknown_asvs_loc=paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/unknown_asvs.txt")
all_asvs_loc=paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/03-dada2/rep-seqs.fna")

n_unknown <- system(paste('grep -c "asv." ',unknown_asvs_loc), intern=T)
n_asvs=system(paste('grep -c "^>" ',all_asvs_loc), intern=T)

cat("In total there were ", n_unknown, " asvs, from a total of ", n_asvs)
```

The used command was:

```{r, echo=F}
if (config$BLAST$include) {
  cat("blastn -query  results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/blca/", config$DATABASE$name, "_unclassified-seqs_cutoff", config$TAXONOMY$blca_confidence_cutoff, ".fna", 
      " -out " , "results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/blast/", config$DATABASE$name, "_unclassified_blast_results_cutoff", config$TAXONOMY$blca_confidence_cutoff, ".tab", 
      " -outfmt 6 -perc_indentity ", config$BLAST$perc_identity, " ", "config$BLAST$database", sep = "")
} else {
  cat("Blast was not included in this analysis run")
}
```

The results of the blast query were then filtered with a last common ancestor script using [basta](https://github.com/timkahlke/BASTA). BASTA is a command-line tool written in python that was developed to enable a basic taxonomy annotation based on a Last Common Ancestor algorithm similar to the one implemented in MEGAN. 
A number of parameters can be chosen for the basta-lca analysis. The following command was used. 

```{r, echo=F}
if (config$BLAST$include) {
  cat("basta sequence -p ", config$BLAST$portion_of_hits, " -i ", config$BLAST$percent_identity, " -l ", config$BLAST$alignment_length, " -e ", config$BLAST$e_value, " -n ", config$BLAST$max_hits, " -m ", config$BLAST$min_hits, " -d ", config$BLAST$tax_db,
  "  results/", config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/blast/", config$DATABASE$name, "_unclassified_blast_results_cutoff", config$TAXONOMY$blca_confidence_cutoff, ".tab   results/", 
  config$PROJECT, "/runs/", config$RUN, "/04-taxonomy/blast/", config$DATABASE$name, "_unclassified_blast_results_lca_cutoff", config$TAXONOMY$blca_confidence_cutoff, ".tab gb", sep="")
} else {
  cat("Blast was not included in this analysis run")
}
```

A phyloseq object was built with this information, and can be found here:

```{r, echo=F, results='asis'}
cat("`./results/", config$PROJECT, "/runs/", config$RUN, "/05-dwca/phyloseq_object.rds`", sep = "")
```

The total information stored in the phyloseq object is:

```{r, echo=F}
pseq <- readRDS(paste0("../../results/", config$PROJECT, "/runs/", config$RUN, "/05-dwca/phyloseq_object.rds"))

pseq
```

Note also that the sequences of the ASVs are stored in the tax_table of the phyloseq object.

The total number of taxa assigned at each taxonomic level:

```{r, echo=F}
if (colSums(is.na(tax_table(pseq)[,c(7)])) == 0) {
  colSums(tax_table(pseq)[,c(1:7)]!="") 
} else {
  colSums(!is.na(tax_table(pseq)[,c(1:7)]))
}
```

In relative abundance (see total number of reads in the dada2 output), the most abundant taxa at phylum level and at species level are shown in the following images.

```{r, echo=F}
# in case there are NA's in the pseq table:
pseq@otu_table <- replace(pseq@otu_table, is.na(pseq@otu_table), 0)

pseq_rel <- transform_sample_counts(pseq, function(x) x / sum(x) )

# Top 10 at phylum level:
pseq_phyla <- tax_glom(pseq, taxrank = "phylum")
top.phyla <- sort(tapply(taxa_sums(pseq), tax_table(pseq)[, "phylum"], sum), TRUE)
top.phyla <- top.phyla[1:10]
pseq_phyla <- subset_taxa(pseq_rel, phylum %in% names(top.phyla))
plot_bar(pseq_phyla, x = "phylum", fill="phylum")+
  geom_bar(stat = "identity")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = -90, hjust = 0), legend.position = 'none', plot.title = element_text(size = 10, face = "bold"))+
  ggtitle(paste("Relative abundance of the most common phyla in the full set of samples (n =", nsamples(pseq), ")"))

# Top 10 at class level:
pmelt <- psmelt(pseq_rel)
pseq_phyla <- tax_glom(pseq, taxrank = "class")
top.phyla <- sort(tapply(taxa_sums(pseq), tax_table(pseq)[, "class"], sum), TRUE)
top.phyla <- top.phyla[1:10]
pseq_phyla <- subset_taxa(pseq_rel, class %in% names(top.phyla))
plot_bar(pseq_phyla, x = "class", fill="class")+
  geom_bar(stat = "identity")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = -90, hjust = 0), legend.position = 'none', plot.title = element_text(size = 10, face = "bold"))+
  ggtitle(paste("Relative abundance of the most common taxonomic classes in the full set of samples (n =", nsamples(pseq), ")"))

# Top 10 at species level:
pmelt <- psmelt(pseq_rel)
pseq_phyla <- tax_glom(pseq, taxrank = "species")
top.phyla <- sort(tapply(taxa_sums(pseq), tax_table(pseq)[, "species"], sum), TRUE)
top.phyla <- top.phyla[1:10]
pseq_phyla <- subset_taxa(pseq_rel, species %in% names(top.phyla))
plot_bar(pseq_phyla, x = "species", fill = "species")+
  geom_bar(stat = "identity")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = -90, hjust = 0), legend.position = 'none', plot.title = element_text(size = 10, face = "bold"))+
  ggtitle(paste("Relative abundance of the most common species in the full set of samples (n =", nsamples(pseq), ")"))
```

The relationship of the biodiversity between the samples is shown in the following ordination. The closer together the samples are depicted, the more similar the biodiversity measured in each of the samples was.
(Works only if more than 2 samples are analysed in a run.)

```{r, echo=F}
pseq <- subset_samples(pseq, sample_sums(pseq)>0)
pseq_rel <- transform_sample_counts(pseq, function(x) x / sum(x) )

if (nsamples(pseq_rel) > 2) {
  pseq.ord <- ordinate(pseq_rel, "PCoA", "bray")
  sample_data(pseq_rel)$sample_id <- sample_names(pseq_rel)

  plot_ordination(pseq_rel, pseq.ord) +
    geom_point(size = 4, fill = "white", shape = 21) +
    geom_text(aes(label = sample_id), size = 4, vjust = "inward", hjust = "inward") +
    theme_bw()
}
```

## Darwin core archive tables

Finally, the results of this analysis were collected in a Darwin Core Occurrence table and DNA-derived data extension, to facilitate submitting the data to OBIS.  

First, lsids for the assigned taxa were collected based on names in the WoRMS database. Occurrences that are not included in the final tables are mostly known taxa that are not marine. These occurrences are listed in the table 05-dwca/Taxa_not_in_worms.tsv. This table also potentially has marine species that are not recognized by WoRMS yet, and therefore it is recommended to manually review this table.

All unknown sequences that did not have a taxonomic assignment are added in the occurrence table as 'Incertae sedis' (urn:lsid:marinespecies.org:taxname:12), however in the future more checks will be made to filter out sequences that were not from the target gene region or are likely not marine.

In addition, if there was a control sample which was labelled in the original sample as OccurrenceStatus; "absent", any occurrences of the taxa found in this sample were removed from all samples.

The final tables can be found at:

```{r, echo=F, results='asis'}
cat("- results/", config$PROJECT, "/runs/", config$RUN, "/05-dwca/Occurrence_table.tsv  \n", sep = "")
cat("- results/", config$PROJECT, "/runs/", config$RUN, "/05-dwca/DNA_extension_table.tsv  \n  \n", sep = "")
```

From this analysis run there were

```{r, echo=F, results='asis'}
occ_path <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN,"/05-dwca/Occurrence_table.tsv")
occ_table <- read.table(occ_path, header = TRUE, sep = "\t")

cat("The total number of occurrences was", nrow(occ_table), ".  \n")
cat("Of which", sum(occ_table$scientificName!="Incertae sedis"), "were known, and therefore", sum(occ_table$scientificName=="Incertae sedis"), "or", sum(occ_table$scientificName=="Incertae sedis") / nrow(occ_table) * 100, "% remained unknown.  \n  \nThe sequences of these occurrences can nevertheless be added to the database, and it is possible that their taxonomy will be defined when reference databases develop.  \n  \n")

notinworms_path <- paste0("../../results/", config$PROJECT, "/runs/", config$RUN,"/05-dwca/taxa_not_in_worms.tsv")
notinworms_table <- read.table(notinworms_path, header = TRUE, sep = "\t")

cat("The number of names not found in the marine records of WoRMS was", nrow(notinworms_table), ".")
```
