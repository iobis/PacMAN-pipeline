"""
Pacman-pipeline
Authors: Saara Suominen,
Last update: 24/06/2021
"""
import pandas as pd

#Better to initiate the run with the configfile, so no need to change the snakefile each time it is run
configfile: "./config/config-pacman.yaml"

PROJECT = config["PROJECT"]
RUN = config["RUN"]
DATABASE = config["DATABASE"]["name"]
CUTOFF=config["TAXONOMY"]["blca_identity_cutoff"]
sample_set = pd.read_csv(config["SAMPLE_SET"], header=0)
samples = pd.unique(sample_set['sample-id'])

print("Analysing samples:")
for iteration, item in enumerate(samples):
  print(iteration+1, ". ", item, sep='')

#Here first define any new/needed commands
#if no target is given at the command line, (rule or target ((output) file)
#Snakemake will define the first rule of the Snakefile as the target.
#Hence, it is best practice to have a rule all at the top of the workflow
#which has all typically desired target files as input files.

rule all:
    input:
        expand("results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz", PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz", PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/fw_fastqc.html", PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/rv_fastqc.html", PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/fw_fastqc.zip", PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/rv_fastqc.zip",  PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/multiqc_{RUN}.html", PROJECT=PROJECT, RUN=RUN),
        #expand("{PROJECT}/{RUN}/trimmed/{samples}/{samples}_1P.fastq.gz",  PROJECT=PROJECT, RUN=RUN, samples=samples)
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1U.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2U.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        #expand("{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/", PROJECT=PROJECT, RUN=RUN, samples=samples),
        #expand("{PROJECT}/runs/{RUN}/dada2/filtered/{samples}/{samples}_1P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        #expand("{PROJECT}/runs/{RUN}/dada2/filtered/{samples}/{samples}_2P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam", PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_rejects.fasta", PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local.sam.blca.out", PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt", PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF),
        expand("results/{PROJECT}/runs/{RUN}/06-report/taxonomy/{DATABASE}_blca_tax_table_{CUTOFF}_report.txt", PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF),
        expand("results/{PROJECT}/runs/{RUN}/05-dwca/Occurence_table.csv",PROJECT=PROJECT, RUN=RUN),
        expand("results/{PROJECT}/runs/{RUN}/report.html",PROJECT=PROJECT, RUN=RUN)


# RULES: INITIATE STRUCTURE ----------------------------------------------------------------

#For future expansions of the pipeline, use if statements. Now we are using library (input folder), and paired end reads with no demultiplexing
#if len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE":


rule init_structure:
    input:
        config["SAMPLE_SET"]
    output:
        r1="results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
        r2="results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz"
    shell:
        "workflow/scripts/init_sample_from_manifest_by_sample.sh "+config["PROJECT"]+" {input} {wildcards.samples} "

#{sample} should be read from the manifest file, {PROJECT} should be read from the configfile

# RULES: QUALITY CONTROL----------------------------------------------------------------

#This section contains all quality control

rule fast_qc:
    """
    Runs QC on raw reads
    """
    input:
        r1="results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
        r2="results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz"
    output:
        o1="results/{PROJECT}/samples/{samples}/qc/fw_fastqc.html",
        o2="results/{PROJECT}/samples/{samples}/qc/rv_fastqc.html",
        s1="results/{PROJECT}/samples/{samples}/qc/fw_fastqc.zip",
        s2="results/{PROJECT}/samples/{samples}/qc/rv_fastqc.zip"
    conda:
        "envs/qc.yaml"
    shell:
        "fastqc {input.r1} {input.r2} -o results/{wildcards.PROJECT}/samples/{wildcards.samples}/qc/"


#MULTIQC creates a report of all generated fastqc files, that can be found in: multiqc_data/multiqc_general_stats
#Also can be viewed in html
#Later check if this can be somehow linked to the report.

rule multiqc:
  input:
    raw_qc_fw = expand("results/{{PROJECT}}/samples/{samples}/qc/fw_fastqc.zip", samples=samples),
    raw_qc_rv = expand("results/{{PROJECT}}/samples/{samples}/qc/rv_fastqc.zip", samples=samples),
  output:
    #raw_multi_html = "{PROJECT}/samples/multiqc.html",
    raw_multi_html = "results/{PROJECT}/samples/multiqc_{RUN}.html"
  conda:
    "envs/qc.yaml"
  shell:
    "multiqc -dd 2 -n {output.raw_multi_html} {input.raw_qc_fw} {input.raw_qc_rv}"
    #"multiqc -dd 2 --pdf -n {output.raw_multi_pdf} {input.raw_qc_fw} {input.raw_qc_rv}"
#Note: there is an automatic pdf report, but this requires the installation of latex

# RULES: SEQUENCE TRIMMING ----------------------------------------------------------------

#Apparently when running trimmomatic like this, it searches only in the current folder.
#adapters manually added to the project folders

rule trimmomatic_pe:
  input:
    r1="results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
    r2="results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz"
  output:
    #out = directory("{PROJECT}/{RUN}/trimmed/{samples}")
    p1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1P.fastq.gz",
    u1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1U.fastq.gz",
    p2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2P.fastq.gz",
    u2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2U.fastq.gz",
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/trimmomatic/{samples}_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
    "trimmomatic PE \
    {input}  \
    {output} \
    ILLUMINACLIP:{config[trimmomatic][ILLUMINACLIP]} \
    MAXINFO:{config[trimmomatic][MAXINFO]} \
    LEADING:{config[trimmomatic][LEADING]} \
    TRAILING:{config[trimmomatic][TRAILING]} \
    {config[trimmomatic][extra_params]} \
    2> {log.f1}"

#Repeat quality control here also?

rule cutadapt_paired:
  input:
    p1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1P.fastq.gz",
    p2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2P.fastq.gz",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz",
    o2 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz",
    #o3 = directory("{PROJECT}/runs/{RUN}/cutadapt/{samples}/"),
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
    "cutadapt  \
    -g {config[cutadapt][forward_primer]} \
    -G {config[cutadapt][reverse_primer]} \
    -A {config[cutadapt][rc_forward_primer]} \
    -a {config[cutadapt][rc_reverse_primer]} \
    -o {output.o1} -p {output.o2} \
    {input.p1} {input.p2} \
    --minimum-length 1 {config[cutadapt][extra_params]} \
     1> {log.f1}"

# Reverse primers in place with: 'echo {config[cutadapt][forward_primer]} | tr ACGTacgt TGCAtgca | rev'
# But check also for all special characters!

rule cutadapt_unpaired:
  input:
    u1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1U.fastq.gz",
    u2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2U.fastq.gz",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1U.fastq.gz",
    o2 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2U.fastq.gz",
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_unpaired_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
        "cutadapt \
        -g {config[cutadapt][forward_primer]} \
        -a {config[cutadapt][rc_reverse_primer]} \
        -o {output.o1} \
        {input.u1} \
        {config[cutadapt][se_extra_params]}; \
        cutadapt \
        -g {config[cutadapt][reverse_primer]} \
        -a {config[cutadapt][rc_forward_primer]} \
        -o {output.o2} \
        {input.u2} \
        --minimum-length 1 {config[cutadapt][se_extra_params]} \
        1> {log.f1}"

#Repeat quality control
#Then we may want to have a rule that checks the results of the multiqc for any big problems.
#dada2 has its own quality control steps, maybe I can use those?

# RULES: ASV INFERENCE WITH DADA2----------------------------------------------------------------

#In this section we run the dada2 pipeline
#Should this be divided into a few separate parts? so that each part can be modified in the configfile if needed.
#Be sure to record how many reads have passed the filters before this step! (should all be in the log files of the other steps, which still need to be saved)
#Still need to add all the params to the configfile and to the script

rule dada2_filter:
  input:
    #files = expand("{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/", PROJECT=PROJECT, RUN=RUN, samples=samples),
    f1 = expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
    #f2 = expand("{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
  output:
    FiltF = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_1P.fastq.gz",PROJECT=PROJECT, RUN=RUN, samples=samples),
    FiltR = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_2P.fastq.gz",PROJECT=PROJECT, RUN=RUN, samples=samples),
    #directory(expand("{PROJECT}/runs/{RUN}/dada2/filtered/{samples}/"), PROJECT=PROJECT, RUN=RUN, samples=samples),
    #stats="{PROJECT}/runs/{RUN}/dada2/dada2_filtering_stats.txt",
  conda:
    "envs/dada2.yaml"
  shell:
    "Rscript ./workflow/scripts/Dada2_FilterAndTrim.R \
    results/{config[PROJECT]}/runs/{config[RUN]}/ \
    {config[DADA2][filterAndTrim][Trunc_len_f]} \
    {config[DADA2][filterAndTrim][Trunc_len_r]} \
    {config[DADA2][filterAndTrim][TruncQ]} \
    {config[DADA2][filterAndTrim][Trim_right]} \
    {config[DADA2][filterAndTrim][Trim_left]} \
    {config[DADA2][filterAndTrim][maxLen]} \
    {config[DADA2][filterAndTrim][minLen]} \
    {config[DADA2][filterAndTrim][maxN]} \
    {config[DADA2][filterAndTrim][minQ]} \
    {config[DADA2][filterAndTrim][MaxEE]} \
    {config[DADA2][filterAndTrim][Rm.phix]} \
    {config[DADA2][filterAndTrim][orient.fwd]} \
    {config[DADA2][filterAndTrim][matchIDs]} \
    {config[DADA2][filterAndTrim][id.sep]} \
    {config[DADA2][filterAndTrim][id.field]} \
    {config[DADA2][filterAndTrim][compress]} \
    {config[DADA2][filterAndTrim][multithread]} \
    {config[DADA2][filterAndTrim][n]} \
    {config[DADA2][filterAndTrim][OMP]} \
    {config[DADA2][filterAndTrim][verbose]} \
    {input.f1}"



rule dada2_ASV:
  input:
    FiltF = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_1P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
    FiltR = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_2P.fastq.gz",PROJECT=PROJECT, RUN=RUN, samples=samples),
  output:
    #o1 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/dada2_stats.txt", PROJECT=PROJECT, RUN=RUN),
    #o2 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt", PROJECT=PROJECT, RUN=RUN),
    #o3 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
    o2="results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt",
    o3="results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
  log:
    o1="results/{PROJECT}/runs/{RUN}/06-report/dada2/dada2_stats.txt",
  conda:
    "envs/dada2.yaml"
  shell:
    "Rscript ./workflow/scripts/Dada2_ASVInference.R \
    results/{config[PROJECT]}/runs/{config[RUN]}/ \
    {config[DADA2][learnERRORS][multithread]} \
    {config[DADA2][learnERRORS][nbases]} \
    {config[DADA2][learnERRORS][randomize]} \
    {config[DADA2][learnERRORS][MAX_CONSIST]} \
    {config[DADA2][learnERRORS][OMEGA_C]} \
    {config[DADA2][learnERRORS][verbose]} \
    {config[DADA2][plotERRORS][nti]} \
    {config[DADA2][plotERRORS][ntj]} \
    {config[DADA2][plotERRORS][obs]} \
    {config[DADA2][plotERRORS][err_out]} \
    {config[DADA2][plotERRORS][err_in]} \
    {config[DADA2][plotERRORS][nominalQ]} \
    {config[DADA2][derepFastq][n]} \
    {config[DADA2][dada][selfConsist]} \
    {config[DADA2][dada][pool]} \
    {config[DADA2][dada][priors]} \
    {config[DADA2][mergePairs][minOverlap]} \
    {config[DADA2][mergePairs][maxMismatch]} \
    {config[DADA2][mergePairs][returnRejects]} \
    {config[DADA2][mergePairs][propagateCol]} \
    {config[DADA2][mergePairs][justConcatenate]} \
    {config[DADA2][mergePairs][trimOverhang]} \
    {config[DADA2][removeBimeraDenovo][method]} \
    {input.FiltF}"


# RULES: TAXONOMIC ASSIGNMENT ----------------------------------------------------------------

#In this part we will perform the taxonomic assignment of the reads with bowtie2 and blca
#Following the strategy of the ANACAPA pipeline

#First we will build the bowtie2 database if it is not given in the configfile:
if config["DATABASE"]["location_bowtie2"] == None:
    rule bowtie2_build:
        input:
          fasta=config["DATABASE"]["fasta"],
          #prefix=expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}", DATABASE=DATABASE),
        output:
          expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.{index}.bt2", DATABASE=DATABASE, index=range(1,5)),
          expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.rev.{index}.bt2",DATABASE=DATABASE, index=range(1,3))
        #params:
          #prefix="resources/bowtie2_dbs/{DATABASE}/{DATABASE}",
          #tmp_dir=config['global_tmp_dir']
        conda:
          "envs/bowtie2.yaml"
        shell:
          "bowtie2-build {input.fasta} resources/bowtie2_dbs/"+config["DATABASE"]["name"]+"/"+config["DATABASE"]["name"]


#Input for bowtie2 depends on if the mapping was given or not: now if statement, could be changed to something else?

if config["DATABASE"]["location_bowtie2"] != None:
    rule bowtie2:
      input:
        rep_seqs=expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
      output:
        o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam",
        o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_rejects.fasta",
      log:
        f1 = "results/{PROJECT}/runs/{RUN}/06-report/bowtie2/bowtie2_{DATABASE}_log.txt"
      conda:
        "envs/bowtie2.yaml"
      shell:
        "bowtie2 -x {config[DATABASE][location_bowtie2]} \
        -f -U {input.rep_seqs} \
        -S {output.o1} \
        --no-hd \
        --no-sq \
        --very-sensitive \
        --local \
        --no-unal \
        -p {config[TAXONOMY][threads]} \
        -k {config[TAXONOMY][distinct_alignments]} \
        --un {output.o2} \
        2> {log.f1}"
else:
    rule bowtie2:
      input:
        expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.{index}.bt2", DATABASE=DATABASE, index=range(1,5)),
        expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.rev.{index}.bt2", DATABASE=DATABASE, index=range(1,3)),
        rep_seqs=expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
      output:
        o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam",
        o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_rejects.fasta",
      log:
        f1 = "results/{PROJECT}/runs/{RUN}/06-report/bowtie2/bowtie2_{DATABASE}_log.txt"
      params:
        ref_idx_base="resources/bowtie2_dbs/{DATABASE}/{DATABASE}",
      conda:
        "envs/bowtie2.yaml",
      shell:
        "bowtie2 -x {params.ref_idx_base} \
        -f -U {input.rep_seqs} \
        -S {output.o1} \
        --no-hd \
        --no-sq \
        --very-sensitive \
        --local \
        --no-unal \
        -p {config[TAXONOMY][threads]} \
        -k {config[TAXONOMY][distinct_alignments]} \
        --un {output.o2} \
        2> {log.f1}"


rule blca:
  input:
    f1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local.sam.blca.out",
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/blca/blca_{DATABASE}_log.txt"
  params:
    m = "workflow/scripts/muscle"
  conda:
    "envs/blca.yaml"
  shell:
    "python ./workflow/scripts/blca_from_bowtie.py \
    -i {input.f1} \
    -r {config[DATABASE][taxa]} \
    -q {config[DATABASE][fasta]} \
    -b {config[TAXONOMY][min_identity]} \
    -l {config[TAXONOMY][min_length]} \
    -p {params.m} \
    -n {config[TAXONOMY][bootstrap_no]} \
    -m {config[TAXONOMY][match_score]} \
    -f {config[TAXONOMY][mismatch_penalty]} \
    -g {config[TAXONOMY][gap_penalty]} \
    -o {output.o1} "


rule filter_taxa:
  input:
    f1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local.sam.blca.out",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local_cutoff{CUTOFF}.sam.blca.out.mod",
    o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt",
  shell:
    "sed -e '1s/^/rowname\ttaxonomy\ttaxonomy_confidence\taccessions\\n/' {input.f1} > {output.o1}; \
    python ./workflow/scripts/reformat_summary_for_r.py \
    {output.o1} \
    {output.o2} \
    {config[TAXONOMY][blca_identity_cutoff]}"

rule classified_taxa:
#This rule is just to print a simple table where the amount of taxa that are classified in the previous step are recorded:
  input:
    "results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt",
  output:
    "results/{PROJECT}/runs/{RUN}/06-report/taxonomy/{DATABASE}_blca_tax_table_{CUTOFF}_report.txt"
  shell:
    "echo 'The number of taxa classified at each level with the provided cutoff: ' > {output}; \
    echo 'Kingdom  Phylum  Class   Order   Family  Genus   Species' >> {output}; \
    grep -Eo 'k__|p__|c__|o__|f__|g__|s__' {input} | sort | uniq -c | sort -nr | awk '{{print $2" "$1}}' | column >> {output}"


# RULES: DATA PACKAGING? ----------------------------------------------------------------

#Package files into a darwin core archive (but optional, should be possible to run the pipeline without it.)
#Occurrence core: csv
#DNA-derived data extension: csv

#Add required metadata through the config file? e.g. submitter name, sequencing platform, long-lat

#Taxa file: "{PROJECT}/{RUN}/taxonomy/identity_filtered/{DATABASE}_blca_tax_table_100.txt" = taxonomy
#Reference sequences: "{PROJECT}/{RUN}/dada2/rep-seqs.fna" = sequence
#ASV table: "{PROJECT}/{RUN}/dada2/seqtab-nochim.txt" = abundance

print("\n-------------    There are three main output files of this run:     -------------\n")
print("1. The otu table at: ", PROJECT, "/runs/", RUN, "/dada2/seqtab-nochim.txt", sep='')
print("2. The taxa table at: ", PROJECT, "/runs/", RUN, "/taxonomy/identity_filtered/", DATABASE, "_blca_tax_table_100.txt", sep='')
print("3. The reference sequences at: ", PROJECT, "/runs/", RUN, "/dada2/rep-seqs.fna\n", sep='')


#Retrieve LSIDs using R. Append LSIDs and the linked DNA sequences to the tax table:

rule get_lsids:
  input:
    f1 = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_100.txt", PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE),
    f2 = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/05-dwca/Taxa_not_in_worms.csv",
    o2 = "results/{PROJECT}/runs/{RUN}/05-dwca/Full_tax_table_with_lsids.csv"
  conda:
    "envs/worrms.yaml"
  shell:
    "Rscript ./workflow/scripts/get_lsids.R \
    results/{wildcards.PROJECT}/runs/{wildcards.RUN}/05-dwca/ \
    {input.f1} \
    {input.f2}"

rule make_dwca:
  input:
    f1 = "results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt",
    f2 = "results/{PROJECT}/runs/{RUN}/05-dwca/Full_tax_table_with_lsids.csv",
    #f3 = "{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/05-dwca/Occurence_table.csv",
    o2 = "results/{PROJECT}/runs/{RUN}/05-dwca/DNA_extension_table.csv",
  conda:
    "envs/dwca.yaml"
  shell:
    "Rscript ./workflow/scripts/format_for_dwc_new.R \
    results/{wildcards.PROJECT}/runs/{wildcards.RUN}/05-dwca/ \
    {input.f1} \
    {input.f2} \
    {config[meta][sampling][sample_data_file]}\
    {config[meta][sequencing][target_gene]} \
    {config[meta][sequencing][subfragment]} \
    {config[meta][sequencing][pcr_primer_forward]} \
    {config[meta][sequencing][pcr_primer_reverse]} \
    {config[meta][sequencing][pcr_primer_name_forw]} \
    {config[meta][sequencing][pcr_primer_name_reverse]} \
    {config[meta][sequencing][pcr_primer_reference]} \
    {config[meta][sequencing][lib_layout]} \
    {config[meta][sequencing][seq_meth]} \
    {config[meta][sequencing][sop]} \
    {config[DATABASE][name]} \
    {config[meta][sequencing][extra_fields]}"

#Make here also a simple overview image to be added to the report?


rule reporting:
  conda:
    "envs/rmd.yaml"
  output:
    "results/{PROJECT}/runs/{RUN}/report.html"
  shell:
    "Rscript -e \"rmarkdown::render('workflow/scripts/Report_PacMAN_Pipeline.Rmd', output_file = '../../results/{PROJECT}/runs/{RUN}/report.html')\""
