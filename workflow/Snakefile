"""
Pacman-pipeline
Authors: Saara Suominen,
Last update: 03/12/2021
"""
import pandas as pd
import os

# Config file defaults to config/config.yaml, but can be overridden using --configfile

DEFAULT_CONFIG = "config/config.yaml"

#configfile: DEFAULT_CONFIG
config_path = workflow.overwrite_configfiles[0] if workflow.overwrite_configfiles else os.path.abspath(DEFAULT_CONFIG)

PROJECT = config["PROJECT"]
RUN = config["RUN"]
DATABASE_rdp = config["Rdp"]["rdp_ref_name"]
DATABASE_vsearch=config["Vsearch"]["vsearch_db_name"]
#CUTOFF = config["TAXONOMY"]["blca_confidence_cutoff"]
sample_set = pd.read_csv(config["SAMPLE_SET"], header=0)
samples = pd.unique(sample_set['sample-id'])

print("Analysing samples:")
for iteration, item in enumerate(samples):
  print(iteration+1, ". ", item, sep='')

# Here first define any new/needed commands
# If no target is given at the command line, (rule or target ((output) file)
# Snakemake will define the first rule of the Snakefile as the target.
# Hence, it is best practice to have a rule all at the top of the workflow
# which has all typically desired target files as input files.

rule all:
    input:
        expand("results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/fw_fastqc.html",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/rv_fastqc.html",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/fw_fastqc.zip",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/rv_fastqc.zip",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/multiqc_{RUN}.html",
               PROJECT=PROJECT, RUN=RUN), 
        expand("results/{PROJECT}/samples/{samples}/fastp/{samples}_1P.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/fastp/{samples}_2P.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),       
        expand("results/{PROJECT}/samples/{samples}/fastp/{samples}_1U.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples), 
        expand("results/{PROJECT}/samples/{samples}/fastp/{samples}_2U.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),      
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1U.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2U.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_1P.fastq.gz",
                PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", 
                PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/annotation_results.txt",
                PROJECT=PROJECT, RUN=RUN),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/vsearch/{DATABASE_vsearch}_vsearch_lcaout.tab",
                PROJECT=PROJECT, RUN=RUN, DATABASE_vsearch=DATABASE_vsearch),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results.tab",
               PROJECT=PROJECT, RUN=RUN) if config["BLAST"]["include"] else [],
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results_lca.tab",
               PROJECT=PROJECT, RUN=RUN) if config["BLAST"]["include"] else [],
        expand("results/{PROJECT}/runs/{RUN}/05-dwca/full_tax_table_with_lsids.tsv",
              PROJECT=PROJECT, RUN=RUN),
        expand("results/{PROJECT}/runs/{RUN}/05-dwca/Occurrence_table.tsv",
               PROJECT=PROJECT, RUN=RUN),
        expand("results/{PROJECT}/runs/{RUN}/06-report/report.html",
               PROJECT=PROJECT, RUN=RUN)


# RULES: INITIATE STRUCTURE ----------------------------------------------------------------

# For future expansions of the pipeline, use if statements. Now we are using library (input folder), and paired end reads with no demultiplexing
#if len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE":

rule init_structure:  
    input:
        config["SAMPLE_SET"]
    output:
        r1 = "results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
        r2 = "results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz",
    conda:
        "envs/python.yaml"
    shell:
        "python workflow/scripts/init_sample_from_manifest_by_sample.py " + \
          config["PROJECT"]+" {input} {wildcards.samples} "

# {sample} should be read from the manifest file, {PROJECT} should be read from the configfile

# RULES: QUALITY CONTROL----------------------------------------------------------------

# This section contains all quality control

rule fast_qc:
    """
    Runs QC on raw reads
    """
    input:
        r1 = "results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
        r2 = "results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz",
    output:
        o1 = "results/{PROJECT}/samples/{samples}/qc/fw_fastqc.html",
        o2 = "results/{PROJECT}/samples/{samples}/qc/rv_fastqc.html",
        s1 = "results/{PROJECT}/samples/{samples}/qc/fw_fastqc.zip",
        s2 = "results/{PROJECT}/samples/{samples}/qc/rv_fastqc.zip",
    conda:
        "envs/qc.yaml"
    shell:
        "fastqc {input.r1} {input.r2} -o results/{wildcards.PROJECT}/samples/{wildcards.samples}/qc/"


# MULTIQC creates a report of all generated fastqc files, that can be found in: multiqc_data/multiqc_general_stats
# Also can be viewed in html
# Later check if this can be somehow linked to the report.

rule multiqc:
  input:
    raw_qc_fw = expand(
      "results/{PROJECT}/samples/{samples}/qc/fw_fastqc.zip", samples=samples, PROJECT=PROJECT),
    raw_qc_rv = expand(
      "results/{PROJECT}/samples/{samples}/qc/rv_fastqc.zip", samples=samples, PROJECT=PROJECT),
  output:
    raw_multi_html = "results/{PROJECT}/samples/multiqc_{RUN}.html"
  conda:
    "envs/qc.yaml"
  shell:
    "multiqc -dd 2 -fn {output.raw_multi_html} {input.raw_qc_fw} {input.raw_qc_rv}"

# Note: there is an automatic pdf report, but this requires the installation of latex

# RULES: SEQUENCE TRIMMING ----------------------------------------------------------------

# Fastp https://github.com/OpenGene/fastp was chosen as it 
# automatically trims illumina adapters as well as poly-G tails from NovaSeq datasets
# This is added to the samples/ folder (shared by all runs)
rule fastp:
  input:
    r1 = "results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
    r2 = "results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz",
  output:
    p1 = "results/{PROJECT}/samples/{samples}/fastp/{samples}_1P.fastq.gz",
    p2 = "results/{PROJECT}/samples/{samples}/fastp/{samples}_2P.fastq.gz",
    u1 = "results/{PROJECT}/samples/{samples}/fastp/{samples}_1U.fastq.gz",
    u2 = "results/{PROJECT}/samples/{samples}/fastp/{samples}_2U.fastq.gz",
    h = "results/{PROJECT}/samples/{samples}/fastp/fastp.html",
    j = "results/{PROJECT}/samples/{samples}/fastp/fastp.json",
  log:
    f1 = "results/{PROJECT}/samples/{samples}/fastp/{samples}_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
    "fastp -i {input.r1} -I {input.r2} -o {output.p1} -O {output.p2} \
    --unpaired1 {output.u1} --unpaired2 {output.u2} \
    -h {output.h} -j {output.j} 2> {log.f1}"

# RULES: ADAPTER TRIMMING ----------------------------------------------------------------

#Run cutadapt separately for both files, as cutadapt paired-end will automatically
#discard any reads that do not have a pair. Run also for any remaining unpaired reads.

rule cutadapt_forward: 
  input:
    p = "results/{PROJECT}/samples/{samples}/fastp/{samples}_1P.fastq.gz",
  output:
    o = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz",
    #o3 = directory("{PROJECT}/runs/{RUN}/cutadapt/{samples}/"),
  log:
    f = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_1P_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
    "cutadapt \
    -g {config[cutadapt][forward_primer]} \
    -a {config[cutadapt][rc_reverse_primer]} \
    -o {output.o} \
    --minimum-length 1 \
    --rc \
    {config[cutadapt][se_extra_params]} \
    {input.p} \
    1> {log.f} "

#To use the same rule with other inputs/outputs etc: 
#use rule a as b with:
    #output:
        #"test2.out"

rule cutadapt_reverse: 
  input:
    p = "results/{PROJECT}/samples/{samples}/fastp/{samples}_2P.fastq.gz",
  output:
    o = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz",
    #o3 = directory("{PROJECT}/runs/{RUN}/cutadapt/{samples}/"),
  log:
    f = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_2P_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
    "cutadapt \
    -g {config[cutadapt][reverse_primer]} \
    -a {config[cutadapt][rc_forward_primer]} \
    -o {output.o} \
    --minimum-length 1 \
    --rc \
    {config[cutadapt][se_extra_params]} \
    {input.p} \
    1> {log.f} "


use rule cutadapt_forward as cutadapt_forward_single with: 
  input:
    p = "results/{PROJECT}/samples/{samples}/fastp/{samples}_1U.fastq.gz",
  output:
    o =  "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1U.fastq.gz",
  log:
    f = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_1U_log.txt"

use rule cutadapt_reverse as cutadapt_reverse_single with: 
  input:
    p = "results/{PROJECT}/samples/{samples}/fastp/{samples}_2U.fastq.gz",
  output:
    o =  "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2U.fastq.gz",
  log:
    f = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_2U_log.txt"


#Dada2 requires reads to be in the right paired sequence, therefore the pairing is checked
#if config["meta"]["sequencing"]["lib_layout"] == "Paired":
rule check_paired:
  input: 
    f1 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz",
    f2 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz",
  output:
    p =  directory("results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}"),
    p1 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_1P.fastq.gz",
    p2 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_2P.fastq.gz",
    u1 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_1U.fastq.gz",
    u2 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_2U.fastq.gz",
  shell:
    "python workflow/scripts/check_paired.py {input.f1} {input.f2} {output.p} {output.p} {output.p}"

#To keep all single reads, concatenate reads that were leftover from fastp and analysed by cutadapt separately
#However, this requires a change in the file structure (1P, 2P, 1U, and 2U) files should be in the same folder
#For the following steps, and this messes up with snakemake.

#rule cat_single_reads_f:
#    input:
#      ua = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_1U.fastq.gz",
#      ub = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1U.fastq.gz",
#    output:
#      o = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_1U.fastq.gz",
#    shell:
#      "cat  {input.ua} {input.ub} > {output.o}"

#use rule cat_single_reads_f as cat_single_reads_r with: 
#    input:
#      ua = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_2U.fastq.gz",
#      ub = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2U.fastq.gz",
#    output:
 #     o = "results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_2U.fastq.gz"

# RULES: ASV INFERENCE ----------------------------------------------------------------

rule dada2_filter:
    input:
        f1 = expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_1P.fastq.gz",
                        PROJECT=PROJECT, RUN=RUN, samples=samples),
        f2 = expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/checked/{samples}/{samples}_2P.fastq.gz",
                        PROJECT=PROJECT, RUN=RUN, samples=samples),
    output:
        FiltF = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_1P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
        FiltR = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_2P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
    conda:
        "envs/dada2.yaml"
    shell:
        "Rscript workflow/scripts/Dada2_FilterAndTrim_combined.R \
        results/{config[PROJECT]}/runs/{config[RUN]}/ \
        {config_path} \
        {input.f1}"

rule dada2_ASV:
    input:
        FiltF = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_1P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
        FiltR = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_2P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
    output:
        o2 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt", PROJECT=PROJECT, RUN=RUN, samples=samples),
        o3 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN, samples=samples),
    log:
        o1 = expand("results/{PROJECT}/runs/{RUN}/06-report/dada2/dada2_stats.txt", PROJECT=PROJECT, RUN=RUN, samples=samples),
    conda:
        "envs/dada2.yaml"
    shell:
        "Rscript ./workflow/scripts/Dada2_ASVInference_Single.R \
        results/{config[PROJECT]}/runs/{config[RUN]}/ \
        {config_path} \
        {input.FiltF}"

# RULES: TAXONOMIC ASSIGNMENTS ----------------------------------------------------------------

# Vsearch is used to find the IDs of sequences with very high similarity in the ref database
rule vsearch:
    input:
        rep_seqs = expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
        vsearch_db=config["Vsearch"]["location_vsearch_db"],
    output:
        o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/vsearch/{DATABASE_vsearch}_vsearch_blast.b6",
        o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/vsearch/{DATABASE_vsearch}_vsearch_lcaout.tab",
        log = "results/{PROJECT}/runs/{RUN}/06-report/vsearch/vsearch_{DATABASE_vsearch}.log",
    conda:
        "envs/vsearch.yaml"
    shell:
        "vsearch --usearch_global {input.rep_seqs}  --db {input.vsearch_db} --id {config[Vsearch][pident]} \
        --blast6out {output.o1} --threads 4  --output_no_hits --query_cov {config[Vsearch][query_cov]}\
        --maxaccepts 100 --maxrejects 100 --maxhits 10 \
        --lcaout {output.o2} --lca_cutoff 0.6 --notrunclabels \
        2> {output.log}"

# RDP is used as the main assignment method
rule rdp_class:
    input:
      rep_seqs = expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
    output:
      o1 = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/rdp/{DATABASE}_rdp.output", PROJECT=PROJECT, RUN=RUN, samples=samples, DATABASE=DATABASE_rdp),
    conda:
      "envs/rdp.yaml"
    shell:
      "rdp_classifier -Xmx16g classify \
      -t {config[Rdp][location_rdp_ref]}  \
      -o {output.o1} \
      -q {input.rep_seqs}"

# RDP results are cleaned and filtered based on the specified cutoff, and the assignments are combined with vsearch results.
rule clean_filter_taxa:
    input: 
      vsearch = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/vsearch/{DATABASE_vsearch}_vsearch_blast.b6", PROJECT=PROJECT, RUN=RUN, DATABASE_vsearch=DATABASE_vsearch),
      vsearch_lca = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/vsearch/{DATABASE_vsearch}_vsearch_lcaout.tab", PROJECT=PROJECT, RUN=RUN, DATABASE_vsearch=DATABASE_vsearch),
      rdp = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/rdp/{DATABASE_rdp}_rdp.output",PROJECT=PROJECT, RUN=RUN, DATABASE_rdp=DATABASE_rdp),
      rep_seqs = expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
    output: 
      o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/annotation_results.txt",
      o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/unknown_asvs.txt",
    conda:
      "envs/r.yaml"
    shell: 
        "Rscript ./workflow/scripts/clean_filter_taxa.R \
        results/{config[PROJECT]}/runs/{config[RUN]}/ \
        {config_path} \
        {input.rdp}    \
        {input.vsearch} \
        {input.vsearch_lca} \
        {input.rep_seqs}"


# The remaining fully unknown sequences from rdp are blasted against nt, so that all possible public information is checked.
if config["BLAST"]["include"] == True:
    rule blast_unknown:
        input:
            unknowns = "results/{PROJECT}/runs/{RUN}/04-taxonomy/unknown_asvs.txt",
            fasta = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
        output:
            o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/unclassified-seqs.fna",
            o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results.tab",
        threads: 4
        conda:
            "envs/seqs.yaml",
        shell:
            "seqtk subseq {input.fasta} {input.unknowns}  > {output.o1}; \
            blastn \
            -query {output.o1} \
            -out {output.o2}  \
            -outfmt 6 \
            -perc_identity {config[BLAST][percent_identity]} \
            -qcov_hsp_perc {config[BLAST][percent_coverage]} \
            -num_threads {threads} \
            {config[BLAST][database]}" 

    if config["BLAST"]["tax_db"] != None:
        rule lca_blast:
            input:
                blast_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results.tab",
            output:
                basta_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results_lca.tab"
            conda:
                "envs/seqs.yaml"
            shell:
                "basta sequence \
                -p {config[BLAST][portion_of_hits]} \
                -i {config[BLAST][percent_identity]} \
                -l {config[BLAST][alignment_length]} \
                -e {config[BLAST][e-value]} \
                -n {config[BLAST][max_hits]} \
                -m {config[BLAST][min_hits]} \
                -d {config[BLAST][tax_db]} \
                {input.blast_results} \
                {output.basta_results} \
                gb"
    else:
        rule lca_blast:
            input:
                blast_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results.tab",
            output:
                basta_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results_lca.tab"
            conda:
                "envs/seqs.yaml"
            shell:
                "basta download gb -d ./resources/tax_db/; \
                basta sequence \
                -p {config[BLAST][portion_of_hits]} \
                -i {config[BLAST][percent_identity]} \
                -l {config[BLAST][alignment_length]} \
                -e {config[BLAST][e-value]} \
                -n {config[BLAST][max_hits]} \
                -m {config[BLAST][min_hits]} \
                -d ./resources/tax_db/ \
                {input.blast_results} \
                {output.basta_results} \
                gb"

    rule filter_blast_species:
            input: 
              blast_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results.tab",
              blast_lca = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results_lca.tab",
              fasta = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
            output: 
              o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results_lca_mod.tab",
            conda:
              "envs/r.yaml"
            shell: 
              "Rscript ./workflow/scripts/remove_low_id_blast_hits.R \
              {input.blast_results}    \
              {input.blast_lca} \
              {input.fasta} \
              {output.o1}"



# RULES: FORMAT DATA ----------------------------------------------------------------

rule merge_blast:
  input:
    f1 = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/annotation_results.txt", PROJECT=PROJECT, RUN=RUN),
    f2 = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
    f3 = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/unclassified_blast_results_lca_mod.tab", PROJECT=PROJECT, RUN=RUN) if config["BLAST"]["include"] else [],
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/annotation_results_blast.txt",
  conda:
    "envs/r.yaml"
  shell:
    "Rscript ./workflow/scripts/merge_blast.R \
    results/{wildcards.PROJECT}/runs/{wildcards.RUN}/04-taxonomy/ \
    {input.f1} \
    {input.f2} \
    {config_path} \
    {input.f3} \
    {config[BLAST][database_date]}"


rule make_tax_table:
  input:
    f1 = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/annotation_results_blast.txt",
                PROJECT=PROJECT, RUN=RUN),
    f2 = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/05-dwca/taxa_not_in_worms.tsv",
    o2 = "results/{PROJECT}/runs/{RUN}/05-dwca/full_tax_table_with_lsids.tsv"
  conda:
    "envs/r.yaml"
  shell:
    "Rscript ./workflow/scripts/make_tax_table.R \
    results/{wildcards.PROJECT}/runs/{wildcards.RUN}/05-dwca/ \
    {input.f1} \
    {input.f2} \
    {config_path}"


rule make_dwca:
  input:
    f1 = "results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt",
    f2 = "results/{PROJECT}/runs/{RUN}/05-dwca/full_tax_table_with_lsids.tsv",
    f3 = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/05-dwca/Occurrence_table.tsv",
    o2 = "results/{PROJECT}/runs/{RUN}/05-dwca/DNA_extension_table.tsv",
    o3 = "results/{PROJECT}/runs/{RUN}/05-dwca/phyloseq_object.rds"
  conda:
    "envs/r.yaml"
  shell:
    "Rscript ./workflow/scripts/make_dwca.R \
    results/{wildcards.PROJECT}/runs/{wildcards.RUN}/05-dwca/ \
    {input.f1} \
    {input.f2} \
    {input.f3} \
    {config[meta][sampling][sample_data_file]} \
    {config_path}"


# Make here also a simple overview image to be added to the report?
# RULES: MAKE REPORT ----------------------------------------------------------------

rule reporting:
  input:
    "results/{PROJECT}/runs/{RUN}/05-dwca/Occurrence_table.tsv",
  conda:
    "envs/r.yaml"
  output:
    "results/{PROJECT}/runs/{RUN}/06-report/report.html"
  shell:
    "Rscript -e \"rmarkdown::render('workflow/scripts/Report_PacMAN_Pipeline.Rmd', output_file = '../../results/{PROJECT}/runs/{RUN}/06-report/report.html', params=list(config='{config_path}'))\""
