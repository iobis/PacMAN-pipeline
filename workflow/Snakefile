"""
Pacman-pipeline
Authors: Saara Suominen,
Last update: 03/12/2021
"""
import pandas as pd
import os

# Config file defaults to config/config.yaml, but can be overridden using --configfile

DEFAULT_CONFIG = "config/config.yaml"

configfile: DEFAULT_CONFIG
config_path = workflow.overwrite_configfiles[0] if workflow.overwrite_configfiles else os.path.abspath(DEFAULT_CONFIG)

PROJECT = config["PROJECT"]
RUN = config["RUN"]
DATABASE = config["DATABASE"]["name"]
CUTOFF = config["TAXONOMY"]["blca_identity_cutoff"]
sample_set = pd.read_csv(config["SAMPLE_SET"], header=0)
samples = pd.unique(sample_set['sample-id'])

print("Analysing samples:")
for iteration, item in enumerate(samples):
  print(iteration+1, ". ", item, sep='')

# Here first define any new/needed commands
# If no target is given at the command line, (rule or target ((output) file)
# Snakemake will define the first rule of the Snakefile as the target.
# Hence, it is best practice to have a rule all at the top of the workflow
# which has all typically desired target files as input files.

rule all:
    input:
        expand("results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/fw_fastqc.html",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/rv_fastqc.html",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/fw_fastqc.zip",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/{samples}/qc/rv_fastqc.zip",
               PROJECT=PROJECT, samples=samples),
        expand("results/{PROJECT}/samples/multiqc_{RUN}.html",
               PROJECT=PROJECT, RUN=RUN),
        #expand("{PROJECT}/{RUN}/trimmed/{samples}/{samples}_1P.fastq.gz",  PROJECT=PROJECT, RUN=RUN, samples=samples)
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1U.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2U.fastq.gz",
               PROJECT=PROJECT, RUN=RUN, samples=samples),
        #expand("{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/", PROJECT=PROJECT, RUN=RUN, samples=samples),
        #expand("{PROJECT}/runs/{RUN}/dada2/filtered/{samples}/{samples}_1P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        #expand("{PROJECT}/runs/{RUN}/dada2/filtered/{samples}/{samples}_2P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
               PROJECT=PROJECT, RUN=RUN),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam",
               PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_rejects.fasta",
               PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local.sam.blca.out",
               PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt",
               PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF),
        expand("results/{PROJECT}/runs/{RUN}/06-report/taxonomy/{DATABASE}_blca_tax_table_{CUTOFF}_report.txt",
               PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF),
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_cutoff{CUTOFF}.tab",
               PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF) if config["BLAST"]["include"] else [],
        expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_lca_cutoff{CUTOFF}.tab",
               PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF) if config["BLAST"]["include"] else [],
        expand("results/{PROJECT}/runs/{RUN}/05-dwca/Occurence_table.csv",
               PROJECT=PROJECT, RUN=RUN),
        expand("results/{PROJECT}/runs/{RUN}/06-report/report.html",
               PROJECT=PROJECT, RUN=RUN)


# RULES: INITIATE STRUCTURE ----------------------------------------------------------------

# For future expansions of the pipeline, use if statements. Now we are using library (input folder), and paired end reads with no demultiplexing
#if len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE":

rule init_structure:
    input:
        config["SAMPLE_SET"]
    output:
        r1 = "results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
        r2 = "results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz"
    conda:
        "envs/init.yaml"
    shell:
        "python workflow/scripts/init_sample_from_manifest_by_sample.py " + \
          config["PROJECT"]+" {input} {wildcards.samples} "

# {sample} should be read from the manifest file, {PROJECT} should be read from the configfile

# RULES: QUALITY CONTROL----------------------------------------------------------------

# This section contains all quality control

rule fast_qc:
    """
    Runs QC on raw reads
    """
    input:
        r1 = "results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
        r2 = "results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz"
    output:
        o1 = "results/{PROJECT}/samples/{samples}/qc/fw_fastqc.html",
        o2 = "results/{PROJECT}/samples/{samples}/qc/rv_fastqc.html",
        s1 = "results/{PROJECT}/samples/{samples}/qc/fw_fastqc.zip",
        s2 = "results/{PROJECT}/samples/{samples}/qc/rv_fastqc.zip"
    conda:
        "envs/qc.yaml"
    shell:
        "fastqc {input.r1} {input.r2} -o results/{wildcards.PROJECT}/samples/{wildcards.samples}/qc/"


# MULTIQC creates a report of all generated fastqc files, that can be found in: multiqc_data/multiqc_general_stats
# Also can be viewed in html
# Later check if this can be somehow linked to the report.

rule multiqc:
  input:
    raw_qc_fw = expand(
      "results/{{PROJECT}}/samples/{samples}/qc/fw_fastqc.zip", samples=samples),
    raw_qc_rv = expand(
      "results/{{PROJECT}}/samples/{samples}/qc/rv_fastqc.zip", samples=samples),
  output:
    #raw_multi_html = "{PROJECT}/samples/multiqc.html",
    raw_multi_html = "results/{PROJECT}/samples/multiqc_{RUN}.html"
  conda:
    "envs/qc.yaml"
  shell:
    "multiqc -dd 2 -n {output.raw_multi_html} {input.raw_qc_fw} {input.raw_qc_rv}"
    #"multiqc -dd 2 --pdf -n {output.raw_multi_pdf} {input.raw_qc_fw} {input.raw_qc_rv}"
# Note: there is an automatic pdf report, but this requires the installation of latex

# RULES: SEQUENCE TRIMMING ----------------------------------------------------------------

# Apparently when running trimmomatic like this, it searches only in the current folder.
# adapters manually added to the project folders

rule trimmomatic_pe:
  input:
    r1 = "results/{PROJECT}/samples/{samples}/rawdata/forward_reads/fw.fastq.gz",
    r2 = "results/{PROJECT}/samples/{samples}/rawdata/reverse_reads/rv.fastq.gz"
  output:
    #out = directory("{PROJECT}/{RUN}/trimmed/{samples}")
    p1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1P.fastq.gz",
    u1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1U.fastq.gz",
    p2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2P.fastq.gz",
    u2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2U.fastq.gz",
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/trimmomatic/{samples}_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
    "trimmomatic PE \
    {input}  \
    {output} \
    ILLUMINACLIP:{config[trimmomatic][ILLUMINACLIP]} \
    MAXINFO:{config[trimmomatic][MAXINFO]} \
    LEADING:{config[trimmomatic][LEADING]} \
    TRAILING:{config[trimmomatic][TRAILING]} \
    {config[trimmomatic][extra_params]} \
    2> {log.f1}"

# Repeat quality control here also?
# Reverse complement primers (MBARI-BOG)
# Somehow reverse complements added as input!
#PRIMER1RC=$( echo ${PRIMER1} | tr "[ABCDGHMNRSTUVWXYabcdghmnrstuvwxy]" "[TVGHCDKNYSAABWXRtvghcdknysaabwxr]" | rev )
#PRIMER2RC=$( echo ${PRIMER2} | tr "[ABCDGHMNRSTUVWXYabcdghmnrstuvwxy]" "[TVGHCDKNYSAABWXRtvghcdknysaabwxr]" | rev )

rule cutadapt_paired:
  input:
    p1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1P.fastq.gz",
    p2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2P.fastq.gz",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz",
    o2 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz",
    #o3 = directory("{PROJECT}/runs/{RUN}/cutadapt/{samples}/"),
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
    "cutadapt  \
    -g {config[cutadapt][forward_primer]} \
    -G {config[cutadapt][reverse_primer]} \
    -A {config[cutadapt][rc_forward_primer]} \
    -a {config[cutadapt][rc_reverse_primer]} \
    -o {output.o1} -p {output.o2} \
    {input.p1} {input.p2} \
    --minimum-length 1 {config[cutadapt][extra_params]} \
     1> {log.f1}"

# Reverse primers in place with: 'echo {config[cutadapt][forward_primer]} | tr ACGTacgt TGCAtgca | rev'
# But check also for all special characters!

rule cutadapt_unpaired:
  input:
    u1 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_1U.fastq.gz",
    u2 = "results/{PROJECT}/runs/{RUN}/01-trimmed/{samples}/{samples}_2U.fastq.gz",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1U.fastq.gz",
    o2 = "results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2U.fastq.gz",
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_unpaired1_log.txt",
    f2 = "results/{PROJECT}/runs/{RUN}/06-report/cutadapt/{samples}_unpaired2_log.txt",
  conda:
    "envs/trim.yaml"
  shell:
      "cutadapt \
        -g {config[cutadapt][forward_primer]} \
        -a {config[cutadapt][rc_reverse_primer]} \
        -o {output.o1} \
        {input.u1} \
        --minimum-length 1 {config[cutadapt][se_extra_params]} \
        1> {log.f1}; \
        cutadapt \
        -g {config[cutadapt][reverse_primer]} \
        -a {config[cutadapt][rc_forward_primer]} \
        -o {output.o2} \
        {input.u2} \
        --minimum-length 1 {config[cutadapt][se_extra_params]} \
        1> {log.f2}"

# Repeat quality control
# Then we may want to have a rule that checks the results of the multiqc for any big problems.
# dada2 has its own quality control steps, maybe I can use those?

# RULES: ASV INFERENCE WITH DADA2----------------------------------------------------------------

# In this section we run the dada2 pipeline, both paired and single reads are analysed and added to the final tables.

if config["meta"]["sequencing"]["lib_layout"] == "Paired":
    rule dada2_filter:
        input:
            #files = expand("{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/", PROJECT=PROJECT, RUN=RUN, samples=samples),
            f1 = expand("results/{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_1P.fastq.gz",
                        PROJECT=PROJECT, RUN=RUN, samples=samples),
            #f2 = expand("{PROJECT}/runs/{RUN}/02-cutadapt/{samples}/{samples}_2P.fastq.gz", PROJECT=PROJECT, RUN=RUN, samples=samples),
        output:
            FiltF = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_1P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
            FiltR = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_2P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
            #directory(expand("{PROJECT}/runs/{RUN}/dada2/filtered/{samples}/"), PROJECT=PROJECT, RUN=RUN, samples=samples),
            #stats="{PROJECT}/runs/{RUN}/dada2/dada2_filtering_stats.txt",
        conda:
            "envs/dada2.yaml"
        shell:
            "Rscript ./workflow/scripts/Dada2_FilterAndTrim_combined.R \
            results/{config[PROJECT]}/runs/{config[RUN]}/ \
            "+config_path+" \
            {input.f1}"

    rule dada2_ASV:
        input:
            FiltF = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_1P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
            FiltR = expand("results/{PROJECT}/runs/{RUN}/03-dada2/filtered/{samples}/{samples}_2P.fastq.gz",
                           PROJECT=PROJECT, RUN=RUN, samples=samples),
        output:
            #o1 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/dada2_stats.txt", PROJECT=PROJECT, RUN=RUN),
            #o2 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt", PROJECT=PROJECT, RUN=RUN),
            #o3 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
            o2 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt", PROJECT=PROJECT, RUN=RUN, samples=samples),
            o3 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN, samples=samples),
            o4 = expand("results/{PROJECT}/runs/{RUN}/03-dada2/mapping/{samples}/{samples}_mapping.txt", PROJECT=PROJECT, RUN=RUN, samples=samples),
        log:
            o1 = expand("results/{PROJECT}/runs/{RUN}/06-report/dada2/dada2_stats.txt", PROJECT=PROJECT, RUN=RUN, samples=samples),
        conda:
            "envs/dada2.yaml"
        shell:
            "Rscript ./workflow/scripts/Dada2_ASVInference_Single.R \
            results/{config[PROJECT]}/runs/{config[RUN]}/ \
            "+config_path+" \
            {input.FiltF}"


# RULES: TAXONOMIC ASSIGNMENT ----------------------------------------------------------------

# In this part we will perform the taxonomic assignment of the reads with bowtie2 and blca
# Following the strategy of the ANACAPA pipeline

# First we will build the bowtie2 database if it is not given in the configfile:
if config["DATABASE"]["location_bowtie2"] == None:
    rule bowtie2_build:
        input:
          fasta = config["DATABASE"]["fasta"],
          #prefix=expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}", DATABASE=DATABASE),
        output:
          expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.{index}.bt2",
                 DATABASE=DATABASE, index=range(1, 5)),
          expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.rev.{index}.bt2",
                 DATABASE=DATABASE, index=range(1, 3))
        #params:
          #prefix="resources/bowtie2_dbs/{DATABASE}/{DATABASE}",
          #tmp_dir=config['global_tmp_dir']
        conda:
          "envs/bowtie2.yaml"
        shell:
          "bowtie2-build {input.fasta} resources/bowtie2_dbs/" + \
            config["DATABASE"]["name"]+"/"+config["DATABASE"]["name"]


# Input for bowtie2 depends on if the mapping was given or not: now if statement, could be changed to something else?

if config["DATABASE"]["location_bowtie2"] != None:
    rule bowtie2:
      input:
        rep_seqs = expand(
          "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
      output:
        o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam",
        o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_rejects.fasta",
      log:
        f1 = "results/{PROJECT}/runs/{RUN}/06-report/bowtie2/bowtie2_{DATABASE}_log.txt"
      conda:
        "envs/bowtie2.yaml"
      shell:
        "bowtie2 -x {config[DATABASE][location_bowtie2]} \
        -f -U {input.rep_seqs} \
        -S {output.o1} \
        --no-hd \
        --no-sq \
        --very-sensitive \
        --local \
        --no-unal \
        -p {config[TAXONOMY][threads]} \
        -k {config[TAXONOMY][distinct_alignments]} \
        --un {output.o2} \
        2> {log.f1}"
else:
    rule bowtie2:
      input:
        expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.{index}.bt2",
               DATABASE=DATABASE, index=range(1, 5)),
        expand("resources/bowtie2_dbs/{DATABASE}/{DATABASE}.rev.{index}.bt2",
               DATABASE=DATABASE, index=range(1, 3)),
        rep_seqs = expand(
          "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna", PROJECT=PROJECT, RUN=RUN),
      output:
        o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam",
        o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_rejects.fasta",
      log:
        f1 = "results/{PROJECT}/runs/{RUN}/06-report/bowtie2/bowtie2_{DATABASE}_log.txt"
      params:
        ref_idx_base = "resources/bowtie2_dbs/{DATABASE}/{DATABASE}",
      conda:
        "envs/bowtie2.yaml",
      shell:
        "bowtie2 -x {params.ref_idx_base} \
        -f -U {input.rep_seqs} \
        -S {output.o1} \
        --no-hd \
        --no-sq \
        --very-sensitive \
        --local \
        --no-unal \
        -p {config[TAXONOMY][threads]} \
        -k {config[TAXONOMY][distinct_alignments]} \
        --un {output.o2} \
        2> {log.f1}"


rule blca:
  input:
    f1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/bowtie2/{DATABASE}_bowtie2_local.sam",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local.sam.blca.out",
  log:
    f1 = "results/{PROJECT}/runs/{RUN}/06-report/blca/blca_{DATABASE}_log.txt"
  conda:
    "envs/blca.yaml"
  shell:
    "python ./workflow/scripts/blca_from_bowtie.py \
    -i {input.f1} \
    -r {config[DATABASE][taxa]} \
    -q {config[DATABASE][fasta]} \
    -b {config[TAXONOMY][min_identity]} \
    -l {config[TAXONOMY][min_length]} \
    -n {config[TAXONOMY][bootstrap_no]} \
    -m {config[TAXONOMY][match_score]} \
    -f {config[TAXONOMY][mismatch_penalty]} \
    -g {config[TAXONOMY][gap_penalty]} \
    -o {output.o1} "

rule filter_taxa:
  input:
    f1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local.sam.blca.out",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local_cutoff{CUTOFF}.sam.blca.out.mod",
    o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt",
  shell:
    "sed -e '1s/^/rowname\ttaxonomy\ttaxonomy_confidence\taccessions\\n/' {input.f1} > {output.o1}; \
    python ./workflow/scripts/reformat_summary_for_r.py \
    {output.o1} \
    {output.o2} \
    {config[TAXONOMY][blca_identity_cutoff]}"


# The remaining unknown sequences from blca are blasted against nt, so that all possible public information is checked.
if config["BLAST"]["include"] == True:
    rule blast_unknown:
        input:
            taxa = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local.sam.blca.out",
            filtered = "results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt",
            fasta = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
        output:
            o1 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_bowtie2_local_unknowns_cutoff{CUTOFF}.sam.blca.out",
            o2 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blca/{DATABASE}_unclassified-seqs_cutoff{CUTOFF}.fna",
            o3 = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_cutoff{CUTOFF}.tab",
            #o2 = add information to the sam.blca.out table.
        conda:
            "envs/seqs.yaml",
        shell:
            "awk '/Unclassified/ {{print $1}}' {input.taxa} > {output.o1}; \
            awk '/;;;;;;/ {{print $1}}' {input.filtered} | cat >> {output.o1}; \
            sort -t . -k 2n -o {output.o1} {output.o1}; \
            seqtk subseq {input.fasta} {output.o1}  > {output.o2}; \
            blastn \
            -query {output.o2} \
            -out {output.o3} \
            -outfmt 6 \
            {config[BLAST][database]}"
            # removed this as can be filtered later: -evalue 5e-13 \

# Here we add those that were completely unclassified (no matches in the provided reference database),
# As well as those that did not have a good match in the database (lower confidence tha the cutoff value at Kingdom)

    if config["BLAST"]["tax_db"] != None:
        rule lca_blast:
            input:
                blast_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_cutoff{CUTOFF}.tab",
            output:
                basta_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_lca_cutoff{CUTOFF}.tab"
            conda:
                "envs/seqs.yaml"
            shell:
                "basta sequence \
                -p {config[BLAST][portion_of_hits]} \
                -i {config[BLAST][percent_identity]} \
                -e {config[BLAST][e-value]} \
                -n {config[BLAST][max_hits]} \
                -m {config[BLAST][min_hits]} \
                -d {config[BLAST][tax_db]} \
                {input.blast_results} \
                {output.basta_results} \
                gb"
    else:
        rule lca_blast:
            input:
                blast_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_cutoff{CUTOFF}.tab",
            output:
                basta_results = "results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_lca_cutoff{CUTOFF}.tab"
            conda:
                "envs/seqs.yaml"
            shell:
                "basta download gb -d ./resources/tax_db/; \
                basta sequence \
                -p {config[BLAST][portion_of_hits]} \
                -i {config[BLAST][percent_identity]} \
                -e {config[BLAST][e-value]} \
                -n {config[BLAST][max_hits]} \
                -m {config[BLAST][min_hits]} \
                -d ./resources/tax_db/ \
                {input.blast_results} \
                {output.basta_results} \
                gb"


rule classified_taxa:
  # This rule is just to print a simple table where the amount of taxa that are classified in the previous step are recorded:
  input:
    "results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt",
  output:
    "results/{PROJECT}/runs/{RUN}/06-report/taxonomy/{DATABASE}_blca_tax_table_{CUTOFF}_report.txt"
  shell:
    "mkdir -p results/{PROJECT}/runs/{RUN}/06-report/taxonomy; \
    echo 'The number of taxa classified at each level with the provided cutoff: ' > {output}; \
    echo 'Kingdom  Phylum  Class   Order   Family  Genus   Species' >> {output}; \
    grep -Eo 'k__|p__|c__|o__|f__|g__|s__' {input} | sort | uniq -c | sort -nr | awk '{{print $2" "$1}}' | column >> {output}"


# RULES: DATA PACKAGING? ----------------------------------------------------------------

# Package files into a darwin core archive (but optional, should be possible to run the pipeline without it.)
# Occurrence core: csv
# DNA-derived data extension: csv

# Add required metadata through the config file? e.g. submitter name, sequencing platform, long-lat

# Taxa file: "{PROJECT}/{RUN}/taxonomy/identity_filtered/{DATABASE}_blca_tax_table_100.txt" = taxonomy
# Reference sequences: "{PROJECT}/{RUN}/dada2/rep-seqs.fna" = sequence
# ASV table: "{PROJECT}/{RUN}/dada2/seqtab-nochim.txt" = abundance

print("\n-------------    There are three main output files of this run:     -------------\n")
print("1. The otu table at: ", PROJECT, "/runs/",
      RUN, "/dada2/seqtab-nochim.txt", sep='')
print("2. The taxa table at: ", PROJECT, "/runs/", RUN,
      "/taxonomy/identity_filtered/", DATABASE, "_blca_tax_table_", CUTOFF, ".txt", sep='')
print("3. The reference sequences at: ", PROJECT,
      "/runs/", RUN, "/dada2/rep-seqs.fna\n", sep='')


# Retrieve LSIDs using R. Append LSIDs and the linked DNA sequences to the tax table:

rule get_lsids:
  input:
    f1 = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/identity_filtered/{DATABASE}_blca_tax_table_{CUTOFF}.txt",
                PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF),
    f2 = "results/{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
    f3 = expand("results/{PROJECT}/runs/{RUN}/04-taxonomy/blast/{DATABASE}_unclassified_blast_results_lca_cutoff{CUTOFF}.tab",
                PROJECT=PROJECT, RUN=RUN, DATABASE=DATABASE, CUTOFF=CUTOFF) if config["BLAST"]["include"] else [],
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/05-dwca/Taxa_not_in_worms.csv",
    o2 = "results/{PROJECT}/runs/{RUN}/05-dwca/Full_tax_table_with_lsids.csv"
  conda:
    "envs/worrms.yaml"
  shell: #Add here if statement for the
    "Rscript ./workflow/scripts/get_lsids.R \
    results/{wildcards.PROJECT}/runs/{wildcards.RUN}/05-dwca/ \
    {input.f1} \
    {input.f2} \
    {input.f3} \
    {config[BLAST][database_date]}"

rule make_dwca:
  input:
    f1 = "results/{PROJECT}/runs/{RUN}/03-dada2/seqtab-nochim.txt",
    f2 = "results/{PROJECT}/runs/{RUN}/05-dwca/Full_tax_table_with_lsids.csv",
    #f3 = "{PROJECT}/runs/{RUN}/03-dada2/rep-seqs.fna",
  output:
    o1 = "results/{PROJECT}/runs/{RUN}/05-dwca/Occurence_table.csv",
    o2 = "results/{PROJECT}/runs/{RUN}/05-dwca/DNA_extension_table.csv",
  conda:
    "envs/dwca.yaml"
  shell:
    "Rscript ./workflow/scripts/format_for_dwc_new.R \
    results/{wildcards.PROJECT}/runs/{wildcards.RUN}/05-dwca/ \
    {input.f1} \
    {input.f2} \
    {config[meta][sampling][sample_data_file]}\
    {config[meta][sequencing][target_gene]} \
    {config[meta][sequencing][subfragment]} \
    {config[meta][sequencing][pcr_primer_forward]} \
    {config[meta][sequencing][pcr_primer_reverse]} \
    {config[meta][sequencing][pcr_primer_name_forw]} \
    {config[meta][sequencing][pcr_primer_name_reverse]} \
    {config[meta][sequencing][pcr_primer_reference]} \
    {config[meta][sequencing][lib_layout]} \
    {config[meta][sequencing][seq_meth]} \
    {config[meta][sequencing][sop]} \
    {config[DATABASE][name]} \
    {config[meta][sequencing][extra_fields]}"

# Make here also a simple overview image to be added to the report?


rule reporting:
  input:
    "results/{PROJECT}/runs/{RUN}/05-dwca/Occurence_table.csv",
  conda:
    "envs/rmd.yaml"
  output:
    "results/{PROJECT}/runs/{RUN}/06-report/report.html"
  shell:
    "Rscript -e \"rmarkdown::render('workflow/scripts/Report_PacMAN_Pipeline.Rmd', output_file = '../../results/{PROJECT}/runs/{RUN}/06-report/report.html', params=list(config='"+config_path+"'))\""
